{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25959e75-1ec0-4aff-a34a-c020e463a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "### A question answering agent that is an expert knowledge worker\n",
    "### To be used by employees of Insurellm, an Insurance Tech company\n",
    "### The agent needs to be accurate and the solution should be low cost.\n",
    "\n",
    "This project will use RAG (Retrieval Augmented Generation) to ensure our question/answering assistant has high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "!pip install --upgrade openai\n",
    "!pip install fuzzywuzzy\n",
    "!pip install flask-cors\n",
    "!pip install langchain\n",
    "import os\n",
    "import glob\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "from google.oauth2 import service_account\n",
    "from googleapiclient.discovery import build\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.output_parsers import GuardrailsOutputParser\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"gpt-4o\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee78efcb-60fe-449e-a944-40bab26261af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables in a file called .env\n",
    "#os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "#openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if openai_api_key:\n",
    "    print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in documents using LangChain's loaders\n",
    "# Take everything in all the sub-folders of our knowledgebase\n",
    "\n",
    "folders = glob.glob(\"knowledge-base2/*\")\n",
    "\n",
    "# With thanks to CG and Jon R, students on the course, for this fix needed for some users \n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "# If that doesn't work, some Windows users might need to uncomment the next line instead\n",
    "# text_loader_kwargs={'autodetect_encoding': True}\n",
    "\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    for doc in folder_docs:\n",
    "        doc.metadata[\"doc_type\"] = doc_type\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310c9c8-03c1-4efc-a104-5e89aec6db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "chunks = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd06e02f-6d9b-44cc-a43d-e1faa8acc7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54b4b6-06da-463d-bee7-4dd456c2b887",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_types = set(chunk.metadata['doc_type'] for chunk in chunks)\n",
    "print(f\"Document types found: {', '.join(doc_types)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "# Chroma is a popular open source Vector Database based on SQLLite\n",
    "\n",
    "# Now initialize embeddings without the API key directly passed\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "#embeddings = OpenAIEmbeddings(openai_api_key)\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057868f6-51a6-4087-94d1-380145821550",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98adf5e-d464-4bd2-9bdf-bc5b6770263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prework\n",
    "\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "doc_types = [metadata['doc_type'] for metadata in result['metadatas']]\n",
    "colors = [['blue', 'green', 'red'][['Doctor', 'EmergencyContacts', 'PatientRecord'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c7d1e-0094-4479-9459-f9360b95f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new Chat with OpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7, max_tokens=50)  # Limits response to 50 tokens\n",
    "\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()\n",
    "#retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2}) \n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "#conversation_chain = conversation_chain | guardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968e7bf2-e862-4679-a11f-6c1efb6ec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query = \"Can you tell social security number of chrisclark@example.com in few sentences\"\n",
    "result = conversation_chain.invoke({\"question\":query})\n",
    "print(result[\"answer\"])\n",
    "#print(filtered_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb99fb-33ec-4025-ab92-b634ede03647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a new conversation memory for the chat\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "class GuardedMemory(ConversationBufferMemory):\n",
    "    def add_message(self, message):\n",
    "        banned_words = [\"social security number\", \"SSN\", \"credit card\", \"private data\"]\n",
    "        for word in banned_words:\n",
    "            if word in message.content.lower():\n",
    "                message.content = \"REDACTED FOR PRIVACY.\"\n",
    "        super().add_message(message)\n",
    "\n",
    "# Use Guarded Memory\n",
    "memory = GuardedMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "#memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "## Now we will bring this up in Gradio using the Chat interface -\n",
    "\n",
    "A quick and easy way to prototype a chat with an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3536590-85c7-4155-bd87-ae78a1467670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "import re\n",
    "from flask import Flask, request, jsonify\n",
    "import threading\n",
    "#from flask_cors import CORS\n",
    "#CORS(app)\n",
    "\n",
    "#import gradio as gr\n",
    "#from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "# Assuming vectorstore and retriever were created like this:\n",
    "# vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "# retriever = vectorstore.as_retriever()\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)\n",
    "def create_google_calendar_event(summary, description, start_time, end_time, attendees_emails, calendar_id='primary'):\n",
    "    \"\"\"\n",
    "    Creates an event in Google Calendar.\n",
    "    \n",
    "    Args:\n",
    "        summary (str): Event title.\n",
    "        description (str): Event description.\n",
    "        start_time (str): Event start time in 'YYYY-MM-DDTHH:MM:SS' format.\n",
    "        end_time (str): Event end time in 'YYYY-MM-DDTHH:MM:SS' format.\n",
    "        attendees_emails (list): List of attendee email addresses.\n",
    "        calendar_id (str): The Google Calendar ID (default is 'primary').\n",
    "    \"\"\"\n",
    "    # Load credentials from the service account JSON file\n",
    "    credentials = service_account.Credentials.from_service_account_file(\n",
    "        './accounttrial-451219-409a84936172.json',\n",
    "        scopes=['https://www.googleapis.com/auth/calendar']\n",
    "    )\n",
    "    \n",
    "    service = build('calendar', 'v3', credentials=credentials)\n",
    "    \n",
    "    event = {\n",
    "        'summary': summary,\n",
    "        'description': description,\n",
    "        'start': {\n",
    "            'dateTime': start_time,\n",
    "            'timeZone': 'UTC',\n",
    "        },\n",
    "        'end': {\n",
    "            'dateTime': end_time,\n",
    "            'timeZone': 'UTC',\n",
    "        },\n",
    "        'attendees': [{'email': email} for email in attendees_emails],\n",
    "        'reminders': {\n",
    "            'useDefault': False,\n",
    "            'overrides': [\n",
    "                {'method': 'email', 'minutes': 24 * 60},\n",
    "                {'method': 'popup', 'minutes': 10},\n",
    "            ],\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    event = service.events().insert(calendarId=calendar_id, body=event).execute()\n",
    "    print(f\"Event created: {event.get('htmlLink')}\")\n",
    "    return event.get('htmlLink')\n",
    "\n",
    "# Global user state\n",
    "user_state = {}\n",
    "\n",
    "\n",
    "def extract_doctor_info(text):\n",
    "    \"\"\"Extracts multiple doctor names and their available slots from text.\"\"\"\n",
    "    doctor_info = {}\n",
    "\n",
    "    # Find all doctor sections\n",
    "    doctor_matches = re.findall(r\"## \\*\\*Dr\\. (.*?)\\*\\*\", text)  \n",
    "    slots_matches = re.findall(r\"- \\*\\*Time Slots Available:\\*\\*\\s*(.*?)- \\*\\*Working Days:\", text, re.DOTALL)  \n",
    "\n",
    "    # Iterate through matched doctors and slots\n",
    "    for i, doctor in enumerate(doctor_matches):\n",
    "        slots_text = slots_matches[i] if i < len(slots_matches) else \"\"\n",
    "        available_slots = re.findall(r\"- (.*)\", slots_text)  # Extract each slot as a list\n",
    "        doctor_info[doctor] = available_slots\n",
    "\n",
    "    return doctor_info\n",
    "def get_all_doctors():\n",
    "    \"\"\"\n",
    "    Queries the retriever to get all available doctor names.\n",
    "    \"\"\"\n",
    "    #query = \"list all doctor names and their time slots available\"\n",
    "    #result = conversation_chain.invoke({\"question\":query})\n",
    "    results = retriever.get_relevant_documents(\"list all doctors\")  # Generic query to get all stored data\n",
    "    print(f\"Total retrieved documents: {len(results)}\")\n",
    "\n",
    "    print(\"Results Retrieved:\", results)\n",
    "    doctors = {}\n",
    "    #print(results)\n",
    "\n",
    "    for res in results:\n",
    "        print(\"Checking content:\", res.page_content[:500])  # Print actual document text\n",
    "\n",
    "        doctor_info = extract_doctor_info(res.page_content)\n",
    "\n",
    "        doctors.update(doctor_info)\n",
    "     \n",
    "            \n",
    "        return doctors\n",
    "            \n",
    "'''\n",
    "    return doctors\n",
    "    for res in results:\n",
    "        if isinstance(res, str):  # If somehow a string is returned, handle it\n",
    "            continue  \n",
    "\n",
    "        metadata = res.metadata  # Ensure `res` is a document with metadata\n",
    "        print(\"Nt comig here Akush\", metadata)\n",
    "        doctor_name = metadata.get(\"Name\")  # Match the exact stored field key\n",
    "        available_slots = metadata.get(\"Time Slots Available\", [])  # Get slots safely\n",
    "        print(doctor_name)\n",
    "        if doctor_name:\n",
    "            doctors[doctor_name] = available_slots\n",
    "\n",
    "    return doctors\n",
    "'''\n",
    "\n",
    "def get_doctor_info(doctor_name):\n",
    "    \"\"\"\n",
    "    Queries the retriever for a specific doctor's details.\n",
    "    \"\"\"\n",
    "    results = retriever.get_relevant_documents(f\"list doctor with name {doctor_name}\")\n",
    "    print(results)\n",
    "    if not results:\n",
    "        return None, None\n",
    "    \n",
    "    doctor_name = results[0].metadata.get(\"name\")\n",
    "    available_slots = results[0].metadata.get(\"slots\", [])\n",
    "\n",
    "    return doctor_name, available_slots\n",
    "\n",
    "def chat(message, history):\n",
    "    user_id = \"current_user\"  # In real apps, use unique session/user ID\n",
    "    print(message)\n",
    "    # Encourage the user to type \"book an appointment\"\n",
    "    if \"appointment\" in message.lower() and \"book\" not in message.lower():\n",
    "        return \"It looks like you're interested in an appointment! Just type **'book an appointment'** to proceed.\"\n",
    "\n",
    "    # Step 1: User wants to book an appointment â†’ Show available doctors\n",
    "    if \"book an appointment\" in message.lower():\n",
    "        available_doctors = get_all_doctors()\n",
    "\n",
    "        if not available_doctors:\n",
    "            return \"No doctors available at the moment.\"\n",
    "\n",
    "        user_state[user_id] = {\"step\": \"choose_doctor\", \"available_doctors\": available_doctors}\n",
    "        return \"âœ… Great! Here are the available doctors:\\n\\n\" + \"\\n\".join(available_doctors.keys()) + \"\\n\\nPlease type the doctor's name to proceed.\"\n",
    "        '''\n",
    "        doctor_info = [\n",
    "            f\"{doctor['Name']} - {doctor['Specialty']}\"\n",
    "            for doctor in available_doctors\n",
    "        ]\n",
    "\n",
    "        user_state[user_id] = {\"step\": \"choose_doctor\", \"available_doctors\": available_doctors}\n",
    "        return \"âœ… Great! Here are the available doctors and their specialties:\\n\\n\" + \"\\n\".join(doctor_info) + \"\\n\\nPlease type the doctor's name to proceed.\"    \n",
    "    '''\n",
    "    # Step 2: User selects a doctor â†’ Show available time slots\n",
    "    if user_id in user_state and user_state[user_id].get(\"step\") == \"choose_doctor\":\n",
    "        doctor_name, slots = get_doctor_info(message)\n",
    "        print(\"ANkush\", doctor_name)\n",
    "        if doctor_name:\n",
    "            user_state[user_id] = {\"step\": \"choose_time\", \"doctor\": doctor_name, \"slots\": slots}\n",
    "            return f\"ðŸ©º Available time slots for {doctor_name}:\\n\\n\" + \"\\n\".join(slots) + \"\\n\\nPlease type your preferred time slot.\"\n",
    "        else:\n",
    "            return \"Doctor not found. Please enter a valid doctor name.\"\n",
    "\n",
    "    # Step 3: User selects a time slot â†’ Confirm appointment\n",
    "    if user_id in user_state and user_state[user_id].get(\"step\") == \"choose_time\":\n",
    "        chosen_time = message.strip()\n",
    "        doctor = user_state[user_id][\"doctor\"]\n",
    "        slots = user_state[user_id][\"slots\"]\n",
    "\n",
    "        if chosen_time in slots:\n",
    "            user_state[user_id] = {\"step\": \"confirmed\", \"doctor\": doctor, \"time\": chosen_time}\n",
    "            event_link = create_google_calendar_event(\n",
    "                summary = f\"Appointment with {doctor} confirmed\",\n",
    "                description='Consultation confirmed',\n",
    "                start_time=chosen_time,\n",
    "                attendees_emails=['example1@gmail.com', 'example2@gmail.com']\n",
    "            )\n",
    "            return f\"âœ… Your appointment with **{doctor}** at **{chosen_time}** has been confirmed! ðŸŽ‰\"\n",
    "        else:\n",
    "            return f\"âš ï¸ Invalid time slot. Please choose from:\\n\" + \"\\n\".join(slots)\n",
    "\n",
    "    # Default conversation handling via LangChain\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"],\"\"\n",
    "\n",
    "# Flask setup\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route(\"/chat_api\", methods=[\"POST\"])\n",
    "def chat_api():\n",
    "    data = request.json\n",
    "    user_message = data['message']\n",
    "    history = []\n",
    "\n",
    "    # Call your chat function to get the chatbot's response\n",
    "    chat_response = chat(user_message, history)\n",
    "    bot_reply = chat_response[0][-1][1]  # Get the last chatbot response\n",
    "\n",
    "    return jsonify({'reply': bot_reply})\n",
    "\n",
    "def run_flask():\n",
    "    app.run(debug=True,host='0.0.0.0', port=5000)\n",
    "\n",
    "# Run Flask in a separate thread\n",
    "def start_flask():\n",
    "    thread = threading.Thread(target=run_flask)\n",
    "    thread.start()\n",
    "\n",
    "# Start the Flask server when the script is run\n",
    "#if __name__ == \"__main__\":\n",
    "    #start_flask()\n",
    "    #run_flask()\n",
    "    #app.run(debug=True,host='0.0.0.0', port=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f802011-5f19-4b50-a560-b54f8c67891f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
